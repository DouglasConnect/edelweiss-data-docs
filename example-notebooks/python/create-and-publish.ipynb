{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Publish a Dataset\n",
    "\n",
    "This notebook shows the python client library equivalents to the raw HTTP calls described in the [create-and-publish](https://edelweissdata.com/docs/create-and-publish) walkthrough on the offical EdelweissData documentation website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This walkthrough shows you how to create a new dataset from a csv file, including setting description and metadata and publishing the dataset. In later walkthroughs you will learn more about the details of the authentication scheme, the query language etc..\n",
    "\n",
    "In order to create a dataset, you first need to create an In-Progress Dataset. In this stage you can make as many changes to the dataset as you want. Once you are okay with the data in the dataset and want to make the dataset available to others, you can publish it, thus creating a new version.\n",
    "\n",
    "A published dataset is versioned and as such cannot be modified. In order to modify a published dataset you will need to publish a new version by creating another In-Progress Dataset, apply the new changes you'd like to make and then publish a new version.\n",
    "\n",
    "Keep in mind that the old version will still be available. When accessing datasets, you can specify in the URL if you want to retrieve the dataset at a specific version (identified by the integer version number starting at 1) or whatever is the latest published version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Lifecycle Flow**\n",
    "\n",
    "![Dataset life cycle](https://edelweissdata.com/docs/images/dataset-lifecycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "The steps to publish a new Dataset are as follows\n",
    "\n",
    "1. Create a Dataset\n",
    "2. Upload the Data\n",
    "3. Upload or infer the Schema\n",
    "4. Upload Metadata and Description\n",
    "5. Set the visibility of the dataset\n",
    "6. Publish the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API initialization\n",
    "\n",
    "See the [setup notebook](setup.ipynb) for details on how to install, initialize and authorize the library.\n",
    "\n",
    "Creating and publishing datasets requires autentication so the intialization code below calls the `authenticate()` method on the api. This will block the script execution and ask you to visit the given URL where you will have to log in with your EdelweissData user and confirm the access. After doing this once the token is stored in your users home directory so that this confirmation will not be necessary anymore on the client (you can disable this behaviour with the `cache_jwt` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edelweiss_data import API, QueryExpression as Q\n",
    "\n",
    "# Set this to the url of the Edelweiss Data server you want to interact with\n",
    "edelweiss_api_url = 'https://api.edelweissdata.com'\n",
    "\n",
    "api = API(edelweiss_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.authenticate(scopes=[\"exceedQuota\"]) # exceedQuota is an extra permission that you may or may not have that allows you to use more than the usual amount of storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immediate creation and publishing\n",
    "\n",
    "The entire process outlined above can be done in one convenience call. This gives you less flexibilty but is often all you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PublishedDataset '03482aac-9efe-41b4-b680-e8a3fc8166d8':1 - Test dataset 1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (\"trivial.csv\") as f:\n",
    "    metadata = {\"category\": \"test\", \"metadata-dummy-number\": 42.0}\n",
    "    description = \"Test dataset to demonstrate uploading from python. This can use **markdown formatting**.\"\n",
    "    dataset = api.create_published_dataset_from_csv_file(\"Test dataset 1\", f, metadata = metadata, is_public = False, description = description)\n",
    "dataset                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine grained creation and publishing\n",
    "\n",
    "This is closer to what creating and publishing dataset looks like in other languages when interacting directly with the HTTP endpoints where roughly every step outlined above translates to one method call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset\n",
    "\n",
    "To create a dataset we have to supply the name of the dataset and call the create_dataset method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Test dataset 2\"\n",
    "dataset2 = api.create_in_progress_dataset(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to a Dataset\n",
    "\n",
    "Now that we have created a dataset, we need to populate it with data. We need to read the csv and upload it using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trivial.csv\") as f:\n",
    "    dataset2.upload_data(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Schema\n",
    "\n",
    "At this point we have our data stored as CSV in EdelweissData™ . However, It is currently stored as a bunch of string values in the EdelweissData™ .\n",
    "\n",
    "In order to make the data interesting and allow EdelweissData™  make sense of it, we need to supply a schema.\n",
    "\n",
    "The schema defines the datatype of the columns in the data. The data types could be simple Data Types like `string`, `integer` or they could be more advanced datatypes like `DateTime` or [Smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system)\n",
    "\n",
    "Here are the list of Datatypes currently supported\n",
    "\n",
    "| Data Type        | Representation                      |\n",
    "| --               | --                                  |\n",
    "| String           | xsd:string                          |\n",
    "| Url              | xsd:anyURI                          |\n",
    "| Boolean          | xsd:boolean                         |\n",
    "| Integer          | xsd:integer                         |\n",
    "| Float            | xsd:double                          |\n",
    "| DateTime         | xsd:dateTime                        |\n",
    "| Date             | xsd:date                            |\n",
    "| DatasetId        | edelweiss:datasetid                 |\n",
    "| SMILES           | cheminf:CHEMINF_000018              |\n",
    "| Image            | https://schema.org/image            |\n",
    "| Json             | http://edamontology.org/format_3464 |\n",
    "\n",
    "There are currently two ways to define the Schema\n",
    "\n",
    "1. Inference - We can tell Edelweiss to Infer the schema\n",
    "2. Upload Schema - We supply the correct schema as json\n",
    "\n",
    "#### Schema Inference\n",
    "\n",
    "EdelweissData™  can infer the schema based on some heuristics. Schema inference can only infer basic information like the data type. If you use schema inference, consider augmenting the returned schema (e.g. with richer descriptions for each column if you have them) and uploading it again (see the Schema Upload section below for details)\n",
    "\n",
    "#### Schema Upload\n",
    "\n",
    "The schema inference works very well for basic data types, however there are situations where you want fine grained control over the schema. To accomplish this you need to construct an instance of the Schema class or modify one returned by schema inference, or supply a json formatted schema file. \n",
    "\n",
    "Below the schemafile can either be set to a filename for a schema file generated ahead of time or inferred on the fly if schemafile is set to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemafile = None\n",
    "if schemafile is not None:\n",
    "    with open(schemafile) as f:\n",
    "        dataset2.upload_schema_file(f)\n",
    "else:\n",
    "    dataset2.infer_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Metadata and Description\n",
    "\n",
    "We have successfully inferred the schema; at this point we can move on to publish the dataset. To make our dataset more useful though, it is a good idea to to add a few additional pieces of information. They are:\n",
    "\n",
    "1. Description - Markdown textual description to help users understand what the data is about\n",
    "2. Metadata - A Json object that contains pieces of structured metadata that is useful to allow other people to find the dataset. To learn more about how metadata can be used effectively, have a look at the [metadata documentation](metadata)\n",
    "\n",
    "Both items (as well as the name and the schema if you want) can be uploaded in one method call by using the `update()` method on the PublishedDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\"category\": \"test\", \"metadata-dummy-number\": 42.0}\n",
    "description = \"Test dataset to demonstrate uploading from python. This can use **markdown formatting**.\"\n",
    "dataset2.update(metadata = metadata, description = description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dataset visibility\n",
    "\n",
    "As a final step before publishing we have to decide if the dataset should be publicly visible (i.e. even anonymous HTTP request can retrieve the dataset) or access restricted (in which case you can control which users can access the dataset and/or create new versions)\n",
    "\n",
    "The current visibility can be queried on instances of both InProgress and PublishedDatasets by inspecting the is_public property. By default the visibility of a new dataset is set to **public**.\n",
    "\n",
    "You can set the visibility to either public or access restricted by using the `API.change_dataset_visibility()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.change_dataset_visibility(dataset2.id, is_public = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish the Dataset\n",
    "\n",
    "Now that we have a schema for the dataset and added metadata and a description we can publish our dataset. In the publishing step EdelweissData™  will validate the schema and also pre-compute some information about our data.\n",
    "\n",
    "Publishing a Dataset creates a new version of that Dataset. Once published, a version cannot be changed. If you want to update the dataset you can create a new version. The old version will still be available though. In the URL scheme of EdelweissData™ all endpoints that reference published datasets specify either a specific version by number (starting at 1), or the special version string `latest` to indicate that we want to retrieve whatever is the newest version of this dataset.\n",
    "\n",
    "To document the reason behind publishing new version we need to provide a helpful changelog message when we publish a new version. Publishing is achieved by calling the `publish` method and passing the changelog message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_dataset2 = dataset2.publish(\"Initial publish of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First name</th>\n",
       "      <th>Last name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John</td>\n",
       "      <td>Ford</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jane</td>\n",
       "      <td>Ford</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  First name Last name  Age\n",
       "1       John      Ford   60\n",
       "2       Jane      Ford   59"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_dataset2.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
